import pandas as pd

shell.executable("/usr/bin/bash")
shell.prefix("source /home/users/rpatel7/.bashrc; ")

CHROMS=range(1, 23)
POPS=["CEU", "CHB", "GBR", "YRI", "GIH", "ESN", "LWK"]

# Input files needed:
### "data/CADD_bestfit" - Sella lab GitHub download
### "data/hits_for_roshni.csv" - Hakhamanesh DropBox download
### "data/snp_ID_conversion_table.txt" - Hakhamanesh DropBox download
### "data/1KG_sample_info.txt" <- "/oak/stanford/groups/pritch/shared/1KGenomes_hg38/20130606_sample_info.tsv"
### "data/1KGenomes" <- "/oak/stanford/groups/pritch/scratch.copy/groups/pritch/population_atlas/modern/WGS/1KGenomes"
### "data/variation_feature.txt.gz" <- "/oak/stanford/groups/pritch/users/clweiss/Projections_Mirror/ensembl/variation_feature.txt.gz"
### "data/freq_WB" <- "/oak/stanford/groups/pritch/users/hakha/ukbiobank/data/qc_imputed_data/freq_WB"

rule all:
    input:
        # expand("data/{snp_type}_DAF_trimmed.txt", snp_type=["gwas", "control"])
        expand("data/gwas_null_edited.conditionUKB_WBfreq,CHBfreq.pop{grp}.txt", grp=["YRI"]),
        expand("data/gwas_null_edited.conditionUKB_WBfreq,B.pop{grp}.txt", grp=["CHB"]),
        expand("data/gwas_null_edited.conditionUKB_WBfreq,GIHfreq.pop{grp}.txt", grp=["CHB"]),
        expand("data/conditional_distributions/h5000000_s-1.0e-10_{pop}.txt", pop=["CHB", "YRI"])
        # expand("data/conditional_distributions/pseudoempirical_h0.5_s0.0_{pop}.txt", pop=["CHB", "YRI"]),
        # expand("data/conditional_distributions/pseudoempirical_h0.5_s0.0_quantiles_{pop}.txt", pop=["CHB", "YRI"]),
        # expand("data/conditional_distributions/{datatype}_{pop}.txt", pop=["CHB", "YRI"], datatype=["gwas_hits", "matched_snps"])
        # expand("data/conditional_distributions/{datatype}_quantiles_{pop}.txt", pop=["CHB", "YRI"], datatype=["gwas_hits", "matched_snps"])

rule process_ukb_data: 
    input:
        "data/freq_WB/chr{chr}.afreq"
    output:
        "data/chr{chr}/all_ukb_snps.txt"
    shell:
        """
        cut -f2,5 {input} > {output}
        """

rule sort_rsID_table:
    input:
        "data/snp_ID_conversion_table.txt"
    output:
        "data/snp_ID_conversion_table_sorted.txt"
    shell:
        """
        sort -k1,1 {input} > {output}
        """

rule merge_snps_rsID:
    input:
        snps="data/chr{chr}/all_ukb_snps.txt",
        table=rules.sort_rsID_table.output
    output:
        "data/chr{chr}/all_ukb_snps_rsID.txt"
    shell:
        """
        sort -k1,1 {input.snps} | join -j 1 -t'\t' {input.table} - > {output}
        """

rule resort_snps:
    input:
        rules.merge_snps_rsID.output
    output:
        "data/chr{chr}/all_ukb_snps_rsID_sorted.txt"
    run:
        snps = pd.read_csv(input[0], sep='\t', names=["SNP", "rsID", "alt_freq"])
        snps = snps.drop_duplicates()
        snps["position"] = snps.apply(lambda row: row.SNP.split(':')[1], axis=1)
        snps = snps.sort_values(by="position")
        snps.to_csv(output[0], sep='\t', index=False, columns=["position", "SNP", "rsID", "alt_freq"])   

rule map_B_vals:
    input:
        B="data/CADD_bestfit/chr{chr}.bmap.txt",
        snps=rules.resort_snps.output
    output:
        "data/chr{chr}/all_ukb_snps_rsID_sorted_B.txt"
    shell:
        """
        conda activate py3
        python scripts/map_b_vals.py --snps {input.snps} \
            --bvals {input.B} \
            --out {output}
        conda deactivate
        """

rule process_variation_file:
    input:
        "data/variation_feature.txt.gz"
    output:
        "data/variation_feature.txt"
    shell:
        """
        zcat {input} | cut -f8,9 | grep -v 'N' > {output}
        """

rule aggregate_UKB:
    input:
        expand("data/chr{chr}/all_ukb_snps_rsID_sorted_B.txt", chr=CHROMS)
    output:
        "data/all_ukb_snps_rsID_sorted_B.txt"
    shell:
        """
        head -n 1 {input[0]} > {output}
        tail -n +2 -q {input} >> {output}
        """

rule subset_variation_file:
    input:
        snps=rules.aggregate_UKB.output,
        table="data/variation_feature.txt"
    output:
        "data/variation_feature_subsetted.txt"
    shell:
        """
        conda activate py36
        python scripts/subset_big_table.py --snps {input.snps} \
            --table {input.table} --out {output}
        conda deactivate
        """

rule sort_variation_file:
    input:
        "data/variation_feature_subsetted.txt"
    output:
        "data/variation_feature_sorted.txt"
    shell:
        """
        sort -k2,2 {input} > {output}
        """

rule fetch_ancestral_states:
    input:
        snps=rules.aggregate_UKB.output,
        table=rules.sort_variation_file.output
    output:
        "data/all_ukb_snps_ancestral_allele.txt"
    shell: 
        """
        cut -f2-5 {input.snps} | sort -k2,2 | join -j 2 -t'\t' {input.table} - > {output}
        """ 

rule generate_control_snps:
    input:
        gwas="data/hits_for_roshni.csv",
        snps=rules.fetch_ancestral_states.output
    output:
        gwas="data/gwas.txt",
        control="data/control.txt"
    shell:
        """
        conda activate py36
        python scripts/generate_controls.py --snps {input.snps} \
            --gwas {input.gwas} \
            --out_gwas {output.gwas} \
            --out_control {output.control}
        conda deactivate
        """

rule create_ref_filter:
    input:
        # Using the sample info from hg38 download to map individuals to pops; actual VCFs used are hg19/GRCh37
        "data/1KG_sample_info.txt"
    output:
        "data/filter_{grp}.txt"
    shell:
        """
        grep {wildcards.grp} {input} | cut -f1 > {output}
        """

rule split_chrom:
    input:
        "data/{snp_type}.txt"
    output:
        "data/chr{chr}/chr{chr}_{snp_type}.txt"
    shell:
        """
        grep ^{wildcards.chr}: {input} > {output}
        """

rule create_pos_file:
    input:
        "data/chr{chr}/chr{chr}_{snp_type}.txt"
    output:
        "data/chr{chr}/{snp_type}_positions.txt"
    run:
        df = pd.read_csv(input[0], sep='\t', names=["SNP", "rsID", "alt_freq", "ancestral", "B"])
        df[["chrom", "pos", "ref", "alt"]] = df.apply(lambda row: row.SNP.split(':'), axis=1, result_type="expand")
        df = df.sort_values(by="pos")
        df.to_csv(output[0], columns=["chrom", "pos"], header=False, index=False, sep='\t')

rule extract_positions:
    input:
        pos="data/chr{chr}/{snp_type}_positions.txt",
        vcf="data/1KGenomes/chr{chr}/chr{chr}.filt.1kg.phase3.v5a.biSNPs.vcf.gz"
    output:
        "data/chr{chr}/chr{chr}.extracted_{snp_type}.bcf.gz"
    shell:
        """
        conda activate bcftools-env
        bcftools view -R {input.pos} -T {input.pos} -Ob -o {output} {input.vcf}
        conda deactivate
        """

rule filter_ref:
    input:
        ref=rules.extract_positions.output,
        filter="data/filter_{grp}.txt"
    output:
        bcf="data/chr{chr}/chr{chr}.pop{grp}.extracted_{snp_type}.bcf.gz"
        # idx="data/chr{chr}/chr{chr}.pop{grp}.extracted_{snp_type}.bcf.gz.csi"
    shell:
        """
        conda activate bcftools-env
        bcftools view -S {input.filter} --force-samples -Ou {input.ref} | \
            bcftools view --genotype ^miss --phased -Ob -o {output.bcf}
        conda deactivate
        """

rule calculate_frequency:
    input:
        rules.filter_ref.output.bcf
    output:
        "data/chr{chr}/chr{chr}.pop{grp}.extracted_{snp_type}.frq"
    params:
        prefix="data/chr{chr}/chr{chr}.pop{grp}.extracted_{snp_type}"
    shell:
        """
        conda activate vcftools-env
        vcftools --bcf {input} --freq --out {params.prefix}
        conda deactivate
        """

rule compute_DAF:
    input:
        freq=expand("data/chr{{chr}}/chr{{chr}}.pop{grp}.extracted_{{snp_type}}.frq", grp=POPS),
        anc=rules.split_chrom.output
    output:
        "data/chr{chr}/{snp_type}_DAF.txt"
    shell:
        """
        conda activate py36
        python scripts/merge_frequency_tables.py --ancestral_file {input.anc} \
            --frequency_files {input.freq} \
            --out {output}
        conda deactivate
        """

rule aggregate_snps:
    input:
        expand("data/chr{chr}/{{snp_type}}_DAF.txt", chr=CHROMS)
    output:
        "data/pooled_{snp_type}_DAF.txt"
    shell:
        """
        head -n 1 {input[0]} > {output}
        tail -n +2 -q {input} >> {output}
        """

rule make_JFD_input:
    input:
        rules.aggregate_snps.output
    output:
        "data/{snp_type}_DAF_trimmed.txt"
    shell:
        """
        cut -f 5,10-17 {input} > {output}
        """

rule set_B_bins:
    input:
        "data/pooled_control_DAF.txt"
    output:
        "data/B_bounds.txt"
    shell:
        """
        conda activate py3
        python scripts/bin_b_vals.py --data {input} --out {output}
        conda deactivate
        """

rule control_JFD:
    input:
        data="data/control_DAF_trimmed.txt",
        B=rules.set_B_bins.output
    output:
        "data/conditional_distributions/control.condition{cond}.pop{grp}.npy",
        counts="data/condition{cond}.count_matrix.pop{grp}.npy"
    params:
        out_prefix="data/conditional_distributions/control.condition{cond}.pop",
        var=lambda wildcards: wildcards.cond.split(',')
    shell:
        """
        conda activate py3
        python scripts/compute_empirical_JFD.py --control \
            --data {input.data} \
            --condition {params.var} \
            --B_bounds {input.B} \
            --pop {wildcards.grp} \
            --out_jfd {params.out_prefix} \
            --out_counts {output.counts}
        conda deactivate
        """

rule gwas_JFD:
    input:
        data="data/gwas_DAF_trimmed.txt",
        B=rules.set_B_bins.output,
        control="data/conditional_distributions/control.condition{cond}.pop{grp}.npy"
    output:
        jfd="data/conditional_distributions/gwas.condition{cond}.pop{grp}.npy",
        null="data/gwas_null.condition{cond}.pop{grp}.txt"
    params:
        jfd_prefix="data/conditional_distributions/gwas.condition{cond}.pop",
        null_prefix="data/gwas_null.condition{cond}.pop",
        var=lambda wildcards: wildcards.cond.split(',')
    shell:
        """
        conda activate py3
        python scripts/compute_empirical_JFD.py --gwas \
            --data {input.data} \
            --condition {params.var} \
            --control_jfd {input.control} \
            --out_jfd {params.jfd_prefix} \
            --B_bounds {input.B} \
            --pop {wildcards.grp} \
            --out_null {params.null_prefix}
        conda deactivate
        """

rule process_null:
    input:
        "data/gwas_null.condition{cond}.pop{grp}.txt"
    output:
        "data/gwas_null_edited.condition{cond}.pop{grp}.txt"
    shell:
        """
        conda activate py36
        python scripts/process_null_dist.py --dist {input} --out {output}
        conda deactivate
        """

rule simulated_JFD:
    input:
        sim=expand("data/simulations/ancestral_freq{freq}/h{{h}}_s{{s}}/all_sims.txt", 
                   freq=[i/100 for i in range(1, 100)]),
        ancestral_dist="data/simulations/one_pop/h{h}_s{s}/all_muts.txt"
    output:
        expand("data/conditional_distributions/h{{h}}_s{{s}}_{pop}.txt", pop=["CHB", "YRI"])
    params:
        out_prefix="data/conditional_distributions/h{h}_s{s}_"
    shell:
        """
        conda activate py3
        python scripts/compute_simulation_JFD.py --simulated_data {input.sim} \
            --ancestral_dist {input.ancestral_dist} --out {params.out_prefix}
        conda deactivate
        """

# rule pseudoempirical_JFD:
#     input:
#         data="data/simulations/ooA/h0.5_s0.0/all_muts.txt",
#         JFD=expand("data/conditional_distributions/h0.5_s{s}_{pop}.txt", pop=["CHB", "YRI"], s=[0.0])
#     output:
#         expand("data/conditional_distributions/pseudoempirical_h0.5_s0.0_quantiles_{pop}.txt", pop=["CHB", "YRI"]),
#         expand("data/conditional_distributions/pseudoempirical_h0.5_s0.0_{pop}.txt", pop=["CHB", "YRI"])
#     params:
#         out_prefix="data/conditional_distributions/pseudoempirical_h0.5_s0.0_"
#     shell:
#         """
#         conda activate py36
#         python scripts/compute_empirical_JFD.py --data {input.data} --out {params.out_prefix} \
#             --simulation_JFD {input.JFD}
#         conda deactivate
#         """

# rule extract_rsID_only:
#     input:
#         rules.aggregate_snps.output
#     output:
#         "data/{snp_type}_rsID.txt"
#     shell:
#         """
# 	    cut -f2 -d' ' {input} | grep "rs" > {output}
#         """
# 
# rule preprocess_ancestral_fasta:
#     output:
#         "data/homo_sapiens_ancestor_GRCh38.fa.gz"
#     shell:
#         """
#         conda activate bcftools-env
#         wget -O data/homo_sapiens_ancestor_GRCh38.tar.gz https://ftp.ensembl.org/pub/current_fasta/ancestral_alleles/homo_sapiens_ancestor_GRCh38.tar.gz 
#         tar xfz data/homo_sapiens_ancestor_GRCh38.tar.gz -C data/
#         cat data/homo_sapiens_ancestor_GRCh38/*.fa | bgzip -c > data/homo_sapiens_ancestor_GRCh38.fa.gz
#         rm -r data/homo_sapiens_ancestor_GRCh38
#         rm data/homo_sapiens_ancestor_GRCh38.tar.gz
#         conda deactivate
#         """
# 
# rule reformat_GRCh38:
#     input:
#         # Need to supply data/{snp_type}_rsID.txt files to UCSC table browser (Variation > Common dbSNP) with fields chrom,chromStart,name,ref,alts to obtain this file
#         "data/{snp_type}_GRCh38_raw.txt" 
#     output:
#         "data/{snp_type}_GRCh38_formatted.txt"
#     shell:
#         """
#         echo -e '##fileformat=VCFv4.2' > {output}
#         echo -e '#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT' >> {output}
#         tail -n +2 {input} | sed 's/^chr//' | sed 's/,$//' | awk '{{$2=$2+1}}1' OFS='\t' | sort -k1,1V -k2,2n | sed 's/$/\t100\tPASS\t.\tGT/' >> {output}
#         """
# 
# rule process_raw_vep:
#     input:
#         anc=rules.get_ancestral_states.output.out,
#         snps=rules.aggregate_snps.output
#     output:
#         "data/{snp_type}_ancestral_allele.txt"
#     shell:
#         """
#         conda activate py36
#         python scripts/merge_ancestral_results.py --snp_table {input.snps} \
#             --ancestral_table {input.anc} --out {output}
#         conda deactivate
#         """
